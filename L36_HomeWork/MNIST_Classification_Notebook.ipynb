{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76461d5f",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classification\n",
    "\n",
    "This notebook demonstrates a complete pipeline for classifying handwritten digits from the MNIST dataset using neural networks in Keras/TensorFlow. The workflow follows a modular structure, mirroring the Python project organization.\n",
    "\n",
    "## Project Folder Structure\n",
    "\n",
    "```\n",
    "L36_HomeWork/\n",
    "├── main.py\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── data_loader.py\n",
    "│   ├── model.py\n",
    "│   ├── training.py\n",
    "│   └── evaluation.py\n",
    "├── output/               # auto-created, stores saved plots\n",
    "├── MNIST_Classification_Notebook.ipynb\n",
    "├── README.md\n",
    "└── .gitignore\n",
    "```\n",
    "\n",
    "- **main.py**: Orchestrates the full pipeline.\n",
    "- **src/data_loader.py**: Loads, explores, and preprocesses MNIST data.\n",
    "- **src/model.py**: Defines baseline and improved neural network architectures.\n",
    "- **src/training.py**: Handles model training and training history plotting.\n",
    "- **src/evaluation.py**: Performs evaluation, prediction visualization, error analysis, and confusion matrix plotting.\n",
    "- **output/**: Stores generated plots and images.\n",
    "- **MNIST_Classification_Notebook.ipynb**: This notebook version of the pipeline.\n",
    "- **README.md**: Project documentation.\n",
    "- **.gitignore**: Git ignore rules.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d051b5",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "We load the MNIST dataset using Keras, and print basic statistics about the training and test sets, including shape and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(f\"Train shape: {x_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test shape: {x_test.shape}, {y_test.shape}\")\n",
    "print(f\"Train label distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test label distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9340a6",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We normalize the images to the [0,1] range, flatten each 28x28 image to a 784-dimensional vector, and one-hot encode the labels for use in categorical classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406f0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize, flatten, and one-hot encode\n",
    "def preprocess_data(x, y):\n",
    "    x = x.astype('float32') / 255.0\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    y = to_categorical(y, 10)\n",
    "    return x, y\n",
    "\n",
    "x_train_proc, y_train_proc = preprocess_data(x_train, y_train)\n",
    "x_test_proc, y_test_proc = preprocess_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3bcd11",
   "metadata": {},
   "source": [
    "## Visualize Random Samples\n",
    "\n",
    "Let's visualize 16 random samples from the training set in a 4×4 grid, with their true labels. The figure will also be saved to `output/mnist_samples.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdaef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 16 random samples\n",
    "def visualize_samples(x, y, save_path=None):\n",
    "    idxs = np.random.choice(len(x), 16, replace=False)\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(x[idxs[i]], cmap='gray')\n",
    "        ax.set_title(f\"Label: {y[idxs[i]]}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved sample grid to {save_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "visualize_samples(x_train, y_train, save_path='output/mnist_samples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e3785",
   "metadata": {},
   "source": [
    "## Model Architecture: Baseline Model\n",
    "\n",
    "We define the baseline neural network model as follows:\n",
    "- Dense(512, ReLU) → Dropout(0.2)\n",
    "- Dense(256, ReLU) → Dropout(0.2)\n",
    "- Dense(128, ReLU) → Dropout(0.2)\n",
    "- Dense(10, Softmax)\n",
    "\n",
    "Let's build and display the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faad41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline model\n",
    "def build_model(input_dim=784, num_classes=10):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.2),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "baseline_model = build_model()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0af580",
   "metadata": {},
   "source": [
    "## Model Architecture: Improved Model\n",
    "\n",
    "The improved model adds a 4th hidden layer, increases dropout to 0.3, and uses a lower learning rate (0.0005)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ba4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build improved model\n",
    "def build_improved_model(input_dim=784, num_classes=10):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "improved_model = build_improved_model()\n",
    "improved_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cf5df",
   "metadata": {},
   "source": [
    "## Model Compilation\n",
    "\n",
    "We compile both models using the Adam optimizer, categorical crossentropy loss, and accuracy as the metric. The improved model uses a lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd4df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile models\n",
    "def compile_model(model, lr=0.001):\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "compile_model(baseline_model)\n",
    "compile_model(improved_model, lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c72c20",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We train the baseline model for 20 epochs with a batch size of 128 and a 10% validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee12000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "history = baseline_model.fit(\n",
    "    x_train_proc, y_train_proc,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c862d2",
   "metadata": {},
   "source": [
    "## Plot Training History\n",
    "\n",
    "Let's plot the training and validation loss and accuracy curves. The figure will be saved to `output/training_history.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae36be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "os.makedirs('output', exist_ok=True)\n",
    "plt.savefig('output/training_history.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2cbc2a",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Evaluate the trained baseline model on the test set and print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = baseline_model.evaluate(x_test_proc, y_test_proc, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d607b9",
   "metadata": {},
   "source": [
    "## Single Prediction Visualization\n",
    "\n",
    "Pick a test image of a specific digit (e.g., 7), display the image and a bar chart of softmax probabilities. Save the figure to `output/prediction.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3651f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction for a specific digit (e.g., 7)\n",
    "def predict_single(model, x, y, digit=7, save_path=None):\n",
    "    idxs = np.where(np.argmax(y, axis=1) == digit)[0]\n",
    "    idx = np.random.choice(idxs)\n",
    "    img = x_test[idx].reshape(28, 28)\n",
    "    pred = model.predict(x[idx:idx+1])[0]\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"True: {digit}\")\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(range(10), pred)\n",
    "    plt.title(\"Softmax Probabilities\")\n",
    "    plt.xlabel(\"Digit\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xticks(range(10))\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved prediction to {save_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return np.argmax(pred)\n",
    "\n",
    "predict_single(baseline_model, x_test_proc, y_test_proc, digit=7, save_path='output/prediction.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7ff00",
   "metadata": {},
   "source": [
    "## Error Analysis: Misclassified Images\n",
    "\n",
    "Find all misclassified test images, display a 5×5 grid with predicted (red) and actual (blue) labels. Save the figure to `output/misclassified.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a51716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassified images\n",
    "def analyze_errors(model, x, y_true, save_path=None):\n",
    "    y_pred = np.argmax(model.predict(x), axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    errors = np.where(y_pred != y_true_labels)[0]\n",
    "    print(f\"Total misclassified: {len(errors)}\")\n",
    "    idxs = np.random.choice(errors, min(25, len(errors)), replace=False)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        img = x[idx].reshape(28, 28)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"Pred: {y_pred[idx]}\", color='red')\n",
    "        plt.xlabel(f\"True: {y_true_labels[idx]}\", color='blue')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved misclassified grid to {save_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "analyze_errors(baseline_model, x_test_proc, y_test_proc, save_path='output/misclassified.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2360176",
   "metadata": {},
   "source": [
    "## Plot Confusion Matrix\n",
    "\n",
    "Generate and display a 10×10 confusion matrix heatmap using seaborn. Save the figure to `output/confusion_matrix.png`. Print the most confused digit pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c8076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix and print top confused pairs\n",
    "def plot_confusion_matrix(model, x, y_true, save_path=None):\n",
    "    y_pred = np.argmax(model.predict(x), axis=1)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    cm = confusion_matrix(y_true_labels, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved confusion matrix to {save_path}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    # Print top confused pairs\n",
    "    cm2 = cm.copy()\n",
    "    np.fill_diagonal(cm2, 0)\n",
    "    pairs = np.dstack(np.unravel_index(np.argsort(cm2.ravel())[::-1], (10, 10)))[0]\n",
    "    print(\"Top confused digit pairs:\")\n",
    "    for i in range(5):\n",
    "        a, b = pairs[i]\n",
    "        if cm2[a, b] > 0:\n",
    "            print(f\"{a} vs {b}: {cm2[a, b]} times\")\n",
    "\n",
    "plot_confusion_matrix(baseline_model, x_test_proc, y_test_proc, save_path='output/confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e7fdd",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Now, let's train the improved model and compare both models' test accuracies. We'll display a results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train improved model and compare\n",
    "improved_history = improved_model.fit(\n",
    "    x_train_proc, y_train_proc,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=2\n",
    ")\n",
    "improved_loss, improved_acc = improved_model.evaluate(x_test_proc, y_test_proc, verbose=2)\n",
    "print(f\"Improved model test accuracy: {improved_acc*100:.2f}%\")\n",
    "\n",
    "# Results table\n",
    "print(\"\\n| Model     | Test Accuracy |\")\n",
    "print(\"|-----------|--------------|\")\n",
    "print(f\"| Baseline  | {test_acc*100:.2f}%      |\")\n",
    "print(f\"| Improved  | {improved_acc*100:.2f}%      |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e60dce",
   "metadata": {},
   "source": [
    "## Colab GPU Tip\n",
    "\n",
    "> **Tip:** If running this notebook on [Google Colab](https://colab.research.google.com/), you can enable GPU acceleration for much faster training:\n",
    ">\n",
    "> - Go to `Runtime` > `Change runtime type` > set `Hardware accelerator` to `GPU`.\n",
    "> - Then rerun the notebook cells.\n",
    "\n",
    "This will significantly speed up model training and evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
